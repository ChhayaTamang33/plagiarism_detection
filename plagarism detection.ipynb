{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "data_set= (\"The sky is blue.\", \"The sun is bright.\")\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "term_freq_matrix=count_vect.fit_transform(data_set)\n",
    "#print(count_vect.vocabulary_,'\\n')\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tf_idf_matrix = tfidf.fit_transform(term_freq_matrix)\n",
    "#print (tf_idf_matrix.todense(),'\\n')\n",
    "\n",
    "similarity=cosine_similarity(tf_idf_matrix[0:1], tf_idf_matrix[1:2])\n",
    "print('similarity between the documents is: ',similarity[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the first document here: My name is chhaya\n",
      "Enter the second document here: My name is chitrakala\n",
      "The similarity between 2 documents is:  0.6029748160380572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc1=input('Enter the first document here: ')\n",
    "doc2=input('Enter the second document here: ')\n",
    "data_set=(doc1,doc2)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "term_freq_matrix=count_vect.fit_transform(data_set)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tf_idf_matrix = tfidf.fit_transform(term_freq_matrix)\n",
    "\n",
    "similarity=cosine_similarity(tf_idf_matrix[0], tf_idf_matrix[1])\n",
    "print('The similarity between 2 documents is: ',similarity[0][0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Checking similarity using jaccard similarity\n",
    "STEP 1: importing neccessary toolkit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Taking the text document as input and tokenizing it into sentences and further tokenizing it into words. After that all the punctuation are removed for ease of process. The funtion to convert tokens into lower case is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_segmentation(text):\n",
    "    sent_tokenized = sent_tokenize(text)\n",
    "    return sent_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenization(text):\n",
    "    word_tokenized = []\n",
    "    for sentence in text:\n",
    "        punc_removal = remove_punctuation(word_tokenize(sentence))\n",
    "        word_tokenized.append(punc_removal)\n",
    "    return word_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the context of data storage, serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer) or transmitted and reconstructed later.\n",
    "n-grams is the essential concepts in text mining, which are a set of co-occurring or continuous sequence of n items from a sequence of large text or sentence. Here we define ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(sentences):\n",
    "    res = []\n",
    "    if len(sentences) == 0:\n",
    "        return []\n",
    "    if len(sentences) == 1: # only one sentence\n",
    "        return sentences\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            res.append(token)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(sentences,n):\n",
    "    temp = serialize(sentences)\n",
    "    #print(temp)\n",
    "    trigrams =  nltk.ngrams(temp, n)\n",
    "    trigram_arr = []\n",
    "    for gram in trigrams:\n",
    "        trigram_arr.append(gram)\n",
    "    return trigram_arr\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Stop words are the words in natural language that have a very little meaning, such as \"is\", \"an\", \"the\", etc we remove these words so that it doesnâ€™t end up dominating the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_removal(token_list):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stop_removal_words = [w for w in token_list if w not in stopwords]\n",
    "    return stop_removal_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "def word_counts(word_tokens):\n",
    "    fdist = FreqDist(word_tokens)\n",
    "    keys = fdist.most_common()\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Defining the function to remove puntuation from the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(sentence):\n",
    "    result = []\n",
    "    for token in sentence:\n",
    "        if token not in string.punctuation:\n",
    "            result.append(token)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LCS_document(ori_list,sus_list):\n",
    "    feature = {}\n",
    "    length_s = len(sus_list)\n",
    "    length_o = len(ori_list)\n",
    "    sum_ =0\n",
    "    lcs_len = []\n",
    "    sus_max = []# to find the max for a given specious sentence\n",
    "    sus_num = []\n",
    "    for idx1 in range(length_s):\n",
    "        sus_num = []\n",
    "        for idx2 in range(length_o):\n",
    "            nums = LCS(ori_list[idx2],sus_list[idx1])\n",
    "            sus_num.append(nums)\n",
    "            sum_ +=nums\n",
    "            lcs_len.append(nums)\n",
    "        if len(sus_num) > 1:\n",
    "            temp = max(sus_num)\n",
    "            sus_max.append(temp)\n",
    "    if len(lcs_len) > 1:\n",
    "        max_len = max(lcs_len)\n",
    "    elif len(lcs_len) == 1:\n",
    "        max_len = lcs_len[0]\n",
    "    else:\n",
    "        max_len = 0\n",
    "    sus_sum_nol = 0\n",
    "    if len(sus_max) > 1:\n",
    "        sus_sum_nol = sum(sus_max) / length_s\n",
    "    feature['longest matching sequence'] = max_len\n",
    "    feature['sum of longest matching sequence normalized by the number of suspecious sentence'] = round(sus_sum_nol,4)\n",
    "    feature['average of matching sequence'] = round(sum_ / (length_s*length_o),4)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def LCS_sentence(original_tokens,suspecious_tokens):\n",
    "    length_o = len(original_tokens)\n",
    "    length_s = len(suspecious_tokens)\n",
    "    LCS = [[0 for _ in range(length_s + 1)] for _ in range(length_o + 1)]\n",
    "    res = []\n",
    "    for i in range(1,length_o+1):\n",
    "        for j in range(1,length_s+1):\n",
    "            if original_tokens[i-1] == suspecious_tokens[j-1]:\n",
    "                LCS[i][j] = LCS[i-1][j-1] + 1\n",
    "            else:\n",
    "                LCS[i][j] = max(LCS[i-1][j],LCS[i][j-1])\n",
    "    return LCS[length_o][length_s]\n",
    "\n",
    "X = ['aa','bc','dee']\n",
    "Y = ['aa','dfa','dfa','dee']\n",
    "print(LCS_sentence(X,Y))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "defining function to use jaccard similarity detection code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(x,y,technique=\"Ferret\"):\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = 1\n",
    "    if technique == \"Ferret\":\n",
    "        union_cardinality = len(set.union(*[set(x), set(y)])) #Ferret Comparison Technique(denominator is no of trigrams in two docs i.e. their union)\n",
    "    else:\n",
    "        union_cardinality = len(set(y)) #Containment Measure technique(denominator is no of trigrams in suspicious docs)\n",
    " \n",
    "    return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length(sentences):\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram similarity of documents using ferret technique is:  0.5026455026455027\n",
      "Trigram similarity of documents using containment technique is: 0.6785714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# With the given text we test the code\n",
    "import nltk #error so importing once again\n",
    "nltk.download('punkt')\n",
    "original_text = '''\n",
    "              Plagiarism is known as illegal use of others' part of work or whole work as one's own in any field such as art, \n",
    "              poetry, literature, cinema, research and other creative forms of study. Plagiarism is one of the important issues \n",
    "              in academic and research fields and giving more concern in academic systems. The situation is even worse with the \n",
    "              availability of ample resources on the web. Even though trigram vector consumes comparatively more time, it shows better \n",
    "              results with the labelled data. In addition, the selected trigram vector space model with cosine similarity measure \n",
    "              is compared with tri-gram sequence matching technique with Jaccard measure. In the results, cosine similarity score\n",
    "              shows slightly higher values than the other. Because, it focuses on giving more weight for terms that do not frequently\n",
    "              exist in the dataset and cosine similarity measure using trigram technique is more preferable than the other. \n",
    "              '''\n",
    "suspicious_text = '''\n",
    "                 Plagiarism is understood as illegal use of others' a part of work or whole work as one's own in any field like \n",
    "                 art, poetry, literature, cinema, research and other creative sorts of study. Plagiarism is one among the \n",
    "                 important issues in academic and research fields and giving more concern in academic systems. things is even \n",
    "                 worse with the supply of ample resources on the online. Albeit trigram vector consumes \n",
    "                 comparatively longer , it shows better results with the labelled data. additionally , the chosen trigram vector\n",
    "                 space model with cosine similarity measure is compared with tri-gram sequence matching technique with Jaccard \n",
    "                 measure. within the results, cosine similarity score shows slightly higher values than the opposite . \n",
    "                 Because, it focuses on giving more weight for terms that don't frequently exist within the dataset and \n",
    "                 cosine similarity measure using trigram technique is more preferable than the opposite . \n",
    "                 '''\n",
    "origial_text_lower_case = text_lowercase(original_text)\n",
    "original_sent_tokens = text_segmentation(origial_text_lower_case)\n",
    "original_word_tokens = text_tokenization(original_sent_tokens) # remove punctuation, tokenize each sentence\n",
    "original_trigrams =  generate_ngrams(original_word_tokens,3)\n",
    "\n",
    "suspecious_text_lower_case = text_lowercase(suspicious_text)\n",
    "\n",
    "suspecious_sent_tokens = text_segmentation(suspecious_text_lower_case)\n",
    "suspecious_word_tokens = text_tokenization(suspecious_sent_tokens) # remove punctuation, tokenize each sentence\n",
    "suspecious_trigrams =  generate_ngrams(suspecious_word_tokens,3)\n",
    "sent_length = sentence_length(suspecious_sent_tokens)\n",
    "\n",
    "ferret_trigram_similarity = jaccard_similarity(original_trigrams,suspecious_trigrams,\"Ferret\") #Document Similarity using Ferret Technique\n",
    "containment_trigram_similarity = jaccard_similarity(original_trigrams,suspecious_trigrams,\"Containment\") #Document Similarity using Ferret Technique\n",
    "\n",
    "print(\"Trigram similarity of documents using ferret technique is: \", ferret_trigram_similarity)\n",
    "print(\"Trigram similarity of documents using containment technique is:\", containment_trigram_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
